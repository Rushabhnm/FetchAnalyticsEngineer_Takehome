{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c54a85-8e18-458b-a8b6-d11f9d780c1e",
   "metadata": {},
   "source": [
    "## In this codebase, I will transform raw files from a MongoDB collection by flattening the data, normalizing it, and structuring it into relational tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09af6ac-c0b6-4db3-92a3-4a7133391893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873a2fac-f6b7-493d-bee6-04ec02803866",
   "metadata": {},
   "source": [
    "# 1. Users Raw File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4b44e5-0b73-4213-839a-d6215d2800e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file and loading it \n",
    "\n",
    "users_file_path = \"../raw_files/users.json\"\n",
    "with open(users_file_path, \"r\") as file:\n",
    "    users_data = [json.loads(row) for row in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5961897-8231-4105-ac77-53d2cdc063cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and flattening the JSON data and also handling the missing data by the get dictionary function\n",
    "\n",
    "users_cleansed_data = []\n",
    "for entry in users_data:\n",
    "    users_cleaned_entry = {\n",
    "        \"userID\": entry[\"_id\"][\"$oid\"], \n",
    "        \"signUpSource\": entry.get(\"signUpSource\", None),\n",
    "        \"state\": entry.get(\"state\", None),\n",
    "        \"active\": entry.get(\"active\", None),\n",
    "        \"role\": entry.get(\"role\", None),\n",
    "        \"createdDate\": pd.to_datetime(entry[\"createdDate\"][\"$date\"], unit='ms').date() if \"createdDate\" in entry else None,\n",
    "        \"lastLogin\": pd.to_datetime(entry[\"lastLogin\"][\"$date\"], unit='ms').date() if \"lastLogin\" in entry else None\n",
    "    }\n",
    "    users_cleansed_data.append(users_cleaned_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cd9161-eb7b-4cef-9f1a-c19a9e878320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to DataFrame\n",
    "users_df = pd.DataFrame(users_cleansed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d861a4f1-7e26-49ea-bb3b-3b642a868fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "# finding duplicate enteries \n",
    "duplicated_entries = users_df.duplicated().sum()\n",
    "print(duplicated_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febb06e3-4063-420e-86b2-f983eeb55c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicate enteries\n",
    "users_df = users_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9301f7f-c3f1-437f-a277-7a67db675cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reordering the columns\n",
    "users_df = users_df.reindex(columns=[\"userID\", \"signUpSource\", \"state\", \"active\", \"role\", \"createdDate\", \"lastLogin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5969cb-9744-4395-bc16-d112de2800be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file as a CSV\n",
    "users_df.to_csv(\"../cleansed_file/users_cleansed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769c15e-d42b-4cc2-b58a-63ab4e039803",
   "metadata": {},
   "source": [
    "# 2. Brands Raw FIle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5593f9-2cf4-458f-a91d-237044995fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file and loading it\n",
    "brands_file_path = \"../raw_files/brands.json\"\n",
    "\n",
    "with open(brands_file_path, \"r\") as file:\n",
    "    brands_data = [json.loads(row) for row in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e99aae6-1f1c-47ef-bca8-d49cee2e9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and flattening the JSON data and also handling the missing data by the get dictionary function\n",
    "\n",
    "brands_cleansed_data = []\n",
    "for entry in brands_data:\n",
    "    brands_cleaned_entry = {\n",
    "        \"brandID\": entry.get(\"_id\", {}).get(\"$oid\", None),  \n",
    "        \"brandName\": entry.get(\"name\", None),   \n",
    "        \"barcode\": str(entry.get(\"barcode\", None)),  \n",
    "        \"brandCode\": str(entry.get(\"brandCode\",None)),  \n",
    "        \"category\": entry.get(\"category\", None),  \n",
    "        \"categoryCode\": entry.get(\"categoryCode\", None),  \n",
    "        \"cpgID\": entry.get(\"cpg\", {}).get(\"$id\", {}).get(\"$oid\", None),  \n",
    "        \"cpgRef\": entry.get(\"cpg\", {}).get(\"$ref\", None),  \n",
    "        \"topBrand\": entry.get(\"topBrand\", None)\n",
    "        }\n",
    "    brands_cleansed_data.append(brands_cleaned_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "703ef2e3-9164-4a61-9af8-e504e1a2a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to a df\n",
    "brands_df = pd.DataFrame(brands_cleansed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73760e38-904d-4612-b705-ca6f2e2dcc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# finding duplicate enteries \n",
    "duplicated_entries = brands_df.duplicated().sum()\n",
    "print(duplicated_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67575b72-5a3d-45a4-8091-4be0ff89bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordering the columns\n",
    "brands_df = brands_df.reindex(columns=[\"brandID\",\"barcode\", \"brandName\", \"brandCode\", \"category\", \"categoryCode\", \"cpgID\", \"cpgRef\",\"topBrand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b1b82f-d7a6-41c0-9b15-4684e6729421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file as a CSV\n",
    "brands_df.to_csv(\"../cleansed_file/brands_cleansed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81c36d-4b1f-4d52-b294-f931efcfb7c2",
   "metadata": {},
   "source": [
    "# 3. Receipt Raw File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7f5c8a-0530-4b85-bdae-0659b47718e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file and loading it\n",
    "receipts_file_path = \"../raw_files/receipts.json\"\n",
    "\n",
    "with open(receipts_file_path, \"r\") as file:\n",
    "    receipts_data = [json.loads(row) for row in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39ea13-8244-4d88-a0dc-c1b17e68dd09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ***Note***: Receipt table has rewardsReceiptItemList column which stores all the items in that particular list, hence the granularity of this column when flattened would be on the item level in a single receipt, whereas in the receipt table the granularity is on the receipt level. Hence seperating the rewardsReceiptItemList in a seperate df where receipt would be the foreign key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d009cab-0d07-4c04-be7e-935199c717e9",
   "metadata": {},
   "source": [
    "#### As the keys in the rewardsReceiptItemList are dynamic, collecting all the unique keys, this would be later on used to populate values as each item would have values for its own keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49c53486-d67c-43e6-8ee0-706e8c3e3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all unique keys in rewardsReceiptItemList\n",
    "all_item_keys = set()\n",
    "\n",
    "for receipt in receipts_data:\n",
    "    if \"rewardsReceiptItemList\" in receipt:\n",
    "        for key in receipt[\"rewardsReceiptItemList\"]:\n",
    "            all_item_keys.update(key.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d36e74e-c099-431b-8dfd-7c5cfa0c34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(all_item_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab7ebff-2908-4d62-b576-ffbeacaf3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and flattening the JSON data and also handling the missing data by the get dictionary function\n",
    "\n",
    "receipts_cleansed_data = []\n",
    "items_cleansed_data = []\n",
    "\n",
    "for entry in receipts_data:\n",
    "    receipt_id = entry[\"_id\"][\"$oid\"]\n",
    "\n",
    "    # flattening receipt-level data\n",
    "    receipt_cleaned_entry = {\n",
    "        \"receiptID\": receipt_id,\n",
    "        \"bonusPointsEarned\": entry.get(\"bonusPointsEarned\", None),\n",
    "        \"bonusPointsEarnedReason\": entry.get(\"bonusPointsEarnedReason\", None),\n",
    "        \"createDate\": pd.to_datetime(entry[\"createDate\"][\"$date\"], unit='ms') if \"createDate\" in entry else None,\n",
    "        \"dateScanned\": pd.to_datetime(entry[\"dateScanned\"][\"$date\"], unit='ms') if \"dateScanned\" in entry else None,\n",
    "        \"finishedDate\": pd.to_datetime(entry[\"finishedDate\"][\"$date\"], unit='ms') if \"finishedDate\" in entry else None,\n",
    "        \"modifyDate\": pd.to_datetime(entry[\"modifyDate\"][\"$date\"], unit='ms') if \"modifyDate\" in entry else None,\n",
    "        \"pointsAwardedDate\": pd.to_datetime(entry[\"pointsAwardedDate\"][\"$date\"], unit='ms') if \"pointsAwardedDate\" in entry else None,\n",
    "        \"pointsEarned\": entry.get(\"pointsEarned\", None),\n",
    "        \"purchaseDate\": pd.to_datetime(entry[\"purchaseDate\"][\"$date\"], unit='ms') if \"purchaseDate\" in entry else None,\n",
    "        \"purchasedItemCount\": entry.get(\"purchasedItemCount\", None),\n",
    "        \"rewardsReceiptStatus\": entry.get(\"rewardsReceiptStatus\", None),\n",
    "        \"totalSpent\": entry.get(\"totalSpent\", None),\n",
    "        \"userID\": entry.get(\"userId\", None),\n",
    "    }\n",
    "    receipts_cleansed_data.append(receipt_cleaned_entry)\n",
    "\n",
    "    # flattening item-level data\n",
    "    if \"rewardsReceiptItemList\" in entry:\n",
    "        for item in entry[\"rewardsReceiptItemList\"]:\n",
    "            item_cleaned_entry = {\"receiptID\": receipt_id}  # Link item to receipt\n",
    "            for key in all_item_keys:\n",
    "                item_cleaned_entry[key] = item.get(key, None)  # Fill missing keys with None\n",
    "            items_cleansed_data.append(item_cleaned_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71915df9-64ea-4ea4-887b-c9715113ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to DataFrames\n",
    "receipts_df = pd.DataFrame(receipts_cleansed_data)\n",
    "items_df = pd.DataFrame(items_cleansed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b22fa494-3ac4-4686-9206-5176a15ee980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# finding duplicate enteries \n",
    "duplicated_entries = receipts_df.duplicated().sum()\n",
    "print(duplicated_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0e52561-76d8-4c8a-9d54-a5a491ee1f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file as a CSV\n",
    "receipts_df.to_csv(\"../cleansed_file/cleansed_receipts.csv\", index=False)\n",
    "items_df.to_csv(\"../cleansed_file/cleansed_receipt_items.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
